{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Atv02-PLN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpjBe8cQ10R_"
      },
      "source": [
        "#Atividade Prática 2 - Grupo 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alBONRcrI_vl"
      },
      "source": [
        "Participantes: Antonio Henrique, Pedro Lucca, Samuel Santos e Naum Celestino"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6Ag3CPHovOe"
      },
      "source": [
        "### Introdução"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQI2KLh3UJOb"
      },
      "source": [
        "Este trabalho tem como objetivo a análise textual das contratações emergenciais dos munícipios e Estado do Piauí, realizadas no período de março a setembro de 2020, referentes ao enfrentamento da pandemia da COVID-19. Os dados foram extraídos dos respectivos diários oficiais por uma comissão do Tribunal de Contas do Estado do Piauí, criada especificamente para o acompanhamento dessas contratações, e inseridos em uma planilha Excel. Desta forma, o corpus do trabalho foi construído baseado a partir da coluna “Objeto” da planilha em questão, e assim, será analisado o uso das palavras no corpus, extraindo as principais features que nos ajudam a entender o contexto e saber qual tratamento podemos fazer com o texto.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRoTnG3eQjoj"
      },
      "source": [
        "Tipos de representações de features são medidas estatísticas que servem para indicar a importância de um determinado termo em um documento ou um conjunto de documentos. No tipo de representação binária define-se uma feature para cada palavra, que vai informar se o documento possui ou não essa palavra.\n",
        "\n",
        "Em Term-Frequency o peso de um termo é diretamente proporcional a sua fequencia em um documento. Nele cada documento é representado por um vetor que contêm a quantidade de cada termo.\n",
        "\n",
        "A principal diferença entre Term-frequency(TF) e term frequency inverse document frequency(TF-IDF) é que enquanto TF faz uma analise da frequência das palavras em cada texto, o TF-IDF adiciona a equação o inverso da frequência dessas palavras no Corpus. A idéia é alterar o peso dos termos nos documentos de forma a impedir que termos que sejam muito frequentes em varios documentos diferentes tenham pesos semelhantes a termos que são frequentes em um documento, mas raros no Corpus. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BL4dS-4ozpf"
      },
      "source": [
        "### Resultados\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "uelv73sirgyj",
        "outputId": "38f6afbd-78c6-439e-b76f-47ec7869e052"
      },
      "source": [
        "#Para a leitura do arquivo da segunda atividade\n",
        "import io\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "git_doc = 'https://github.com/PedroLucca/PLN-exercises/blob/master/diarios_oficiais_COVID_classific.xlsx?raw=true'\n",
        "dfs = pd.read_excel(git_doc, sheet_name=None)\n",
        "dfs['diarios_oficiais'].head(5)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Esfera</th>\n",
              "      <th>Diario</th>\n",
              "      <th>Data_do_Diario</th>\n",
              "      <th>Pagina</th>\n",
              "      <th>Tipo_de_contratacao</th>\n",
              "      <th>Num_do_procedimento</th>\n",
              "      <th>Original_Aditivo_Apostilamento</th>\n",
              "      <th>Ratificacao_Extratro_do_contrato_Portaria</th>\n",
              "      <th>Orgao</th>\n",
              "      <th>Municipio</th>\n",
              "      <th>Objeto</th>\n",
              "      <th>Objeto_simplificado</th>\n",
              "      <th>Nome_do_Contratado</th>\n",
              "      <th>CNPJ_CPF</th>\n",
              "      <th>Valor</th>\n",
              "      <th>Num_Contrato</th>\n",
              "      <th>Data_de_assinatura</th>\n",
              "      <th>Prazo_de_Vigencia</th>\n",
              "      <th>Fundamentacao_Legal</th>\n",
              "      <th>Fonte_de_Recurso</th>\n",
              "      <th>Acao_Orcamentaria</th>\n",
              "      <th>Natureza_de_Despesa</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Estadual</td>\n",
              "      <td>DOE - Diário Oficial do Estado</td>\n",
              "      <td>2020-02-19</td>\n",
              "      <td>0029</td>\n",
              "      <td>Dispensa</td>\n",
              "      <td>Não informado</td>\n",
              "      <td>Original</td>\n",
              "      <td>Extrato do contrato</td>\n",
              "      <td>INSTITUTO DE DOENÇAS TROPICAIS NATAN PORTELA</td>\n",
              "      <td>TERESINA</td>\n",
              "      <td>CONTRATAÇAO DE EMPRESA PRESTAÇÃO DE SERVIÇO DE...</td>\n",
              "      <td>Manutenção de veículos e de outros equipamentos</td>\n",
              "      <td>SEMPRE FRIO LTDA</td>\n",
              "      <td>06.837.991/0001-48</td>\n",
              "      <td>133045</td>\n",
              "      <td>001/2020</td>\n",
              "      <td>13/02/2020</td>\n",
              "      <td>12.02.2020 a 27.07.2020</td>\n",
              "      <td>ART. 24, IV DA LEI 8.666/93 e ART. 4º da Lei 1...</td>\n",
              "      <td>100001001</td>\n",
              "      <td>Não informado</td>\n",
              "      <td>Não informado</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Estadual</td>\n",
              "      <td>DOE - Diário Oficial do Estado</td>\n",
              "      <td>2020-03-06</td>\n",
              "      <td>0015</td>\n",
              "      <td>Dispensa</td>\n",
              "      <td>14/2020</td>\n",
              "      <td>Original</td>\n",
              "      <td>Extrato do contrato</td>\n",
              "      <td>HOSPITAL REGIONAL CHAGAS RODRIGUES</td>\n",
              "      <td>PIRIPIRI</td>\n",
              "      <td>CONTRATAÇÃO DE EMPRESA PESSOA JURÍDICA na Aqui...</td>\n",
              "      <td>Aquisição de equipamentos médico-hospitalares</td>\n",
              "      <td>ALFA MED - SISTEMAS MÉDICOS LTDA</td>\n",
              "      <td>11.405.384/0001-49</td>\n",
              "      <td>24450</td>\n",
              "      <td>Não informado</td>\n",
              "      <td>Não informado</td>\n",
              "      <td>180 dias</td>\n",
              "      <td>Art. 24, IV, da Lei 8.666/93 e Lei nº 13.979, ...</td>\n",
              "      <td>Não informado</td>\n",
              "      <td>Não informado</td>\n",
              "      <td>Não informado</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Estadual</td>\n",
              "      <td>DOE - Diário Oficial do Estado</td>\n",
              "      <td>2020-03-06</td>\n",
              "      <td>0016</td>\n",
              "      <td>Dispensa</td>\n",
              "      <td>14/2020</td>\n",
              "      <td>Original</td>\n",
              "      <td>Extrato do contrato</td>\n",
              "      <td>HOSPITAL REGIONAL CHAGAS RODRIGUES</td>\n",
              "      <td>PIRIPIRI</td>\n",
              "      <td>CONTRATAÇÃO DE EMPRESA PESSOA JURÍDICA na Aqui...</td>\n",
              "      <td>Aquisição de equipamentos médico-hospitalares</td>\n",
              "      <td>J R D BRANDÃO EIRELI - MODELO MÓVEIS</td>\n",
              "      <td>23.511.454/0001-22</td>\n",
              "      <td>8185</td>\n",
              "      <td>Não informado</td>\n",
              "      <td>Não informado</td>\n",
              "      <td>180 dias</td>\n",
              "      <td>Art. 24, IV, da Lei 8.666/93 e Lei nº 13.979, ...</td>\n",
              "      <td>Não informado</td>\n",
              "      <td>Não informado</td>\n",
              "      <td>Não informado</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Estadual</td>\n",
              "      <td>DOE - Diário Oficial do Estado</td>\n",
              "      <td>2020-03-06</td>\n",
              "      <td>0016</td>\n",
              "      <td>Dispensa</td>\n",
              "      <td>14/2020</td>\n",
              "      <td>Original</td>\n",
              "      <td>Extrato do contrato</td>\n",
              "      <td>HOSPITAL REGIONAL CHAGAS RODRIGUES</td>\n",
              "      <td>PIRIPIRI</td>\n",
              "      <td>CONTRATAÇÃO DE EMPRESA PESSOA JURÍDICA na Aqui...</td>\n",
              "      <td>Aquisição de equipamentos médico-hospitalares</td>\n",
              "      <td>KSS COMERCIO E INDUSTRIA DE EQUIPAMENTOS MEDIC...</td>\n",
              "      <td>79.805.263/0001-28</td>\n",
              "      <td>14094</td>\n",
              "      <td>Não informado</td>\n",
              "      <td>Não informado</td>\n",
              "      <td>180 dias</td>\n",
              "      <td>Art. 24, IV, da Lei 8.666/93 e Lei nº 13.979, ...</td>\n",
              "      <td>Não informado</td>\n",
              "      <td>Não informado</td>\n",
              "      <td>Não informado</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Estadual</td>\n",
              "      <td>DOE - Diário Oficial do Estado</td>\n",
              "      <td>2020-03-06</td>\n",
              "      <td>0016</td>\n",
              "      <td>Dispensa</td>\n",
              "      <td>14/2020</td>\n",
              "      <td>Original</td>\n",
              "      <td>Extrato do contrato</td>\n",
              "      <td>HOSPITAL REGIONAL CHAGAS RODRIGUES</td>\n",
              "      <td>PIRIPIRI</td>\n",
              "      <td>CONTRATAÇÃO DE EMPRESA PESSOA JURÍDICA na Aqui...</td>\n",
              "      <td>Aquisição de equipamentos médico-hospitalares</td>\n",
              "      <td>MAGNAMED - TECNOLOGIA MÉDICA S/A (FILIAL)</td>\n",
              "      <td>01.298.443/0002-54</td>\n",
              "      <td>143700</td>\n",
              "      <td>Não informado</td>\n",
              "      <td>Não informado</td>\n",
              "      <td>180 dias</td>\n",
              "      <td>Art. 24, IV, da Lei 8.666/93 e Lei nº 13.979, ...</td>\n",
              "      <td>Não informado</td>\n",
              "      <td>Não informado</td>\n",
              "      <td>Não informado</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Esfera  ... Natureza_de_Despesa\n",
              "0  Estadual  ...       Não informado\n",
              "1  Estadual  ...       Não informado\n",
              "2  Estadual  ...       Não informado\n",
              "3  Estadual  ...       Não informado\n",
              "4  Estadual  ...       Não informado\n",
              "\n",
              "[5 rows x 22 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzgDO_j9dP8H",
        "outputId": "a83a8825-c27a-4c5f-c4dd-00421e927c13"
      },
      "source": [
        "diarios = dfs['diarios_oficiais']\n",
        "#corpus = pd.DataFrame(diarios, columns= ['Objeto'])\n",
        "#obj = corpus.head(5).to_numpy()\n",
        "corpus = diarios.get('Objeto').to_numpy().tolist()\n",
        "print(corpus[-5:])\n",
        "corpus_obj_simp = diarios.get('Objeto_simplificado').to_numpy().tolist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Contratação de Empresa para fornecimentos de Teste Rápido para detecção especifica de IGG e IGM do COVID-19, para o Município de Dom Inocêncio - PI.', 'Prestação de contrataçao de profissional para dar suporte técnico ao desenvolvimento do trabalho de: Acompanhamento a Lei Emergenclal Cultural Aldir Biano, sancionada através do Decreto Federal no 14.017/2020; Alimentação dos sistemas e plataformas necessárias; acompanhamento e julgamento dos editais, todo suporte técnico ao Setor de Cultura junto a Secretaria Municipal de Cultura, Esporte e Lazer e a Prefeitura Municipal de São Jose do Piaul - PI.', 'AQUISIÇÃO DE PULVERIZADOR 20 LT FIBRA BICO PLASTICO PARA SECRETARIA MUNICIPAL DE EDUCAÇÃO E ESCOLAS DA REDE ESTADUAL E MUNICIPAL DO MUNICÍPIO DE SÃO JOSÉ DO PIAUÍ· PI.', 'AQUISIÇÃO DE ÁGUA SANITÁRIA I LT, DETERGENTE 500 ML E PAPEL TOALHA PARA SECRETARIA MUNICIPAL DE EDUCAÇÃO E SEUS RESPECTNOS ÓRGÃOS DO MUNICtPIO DE SÃO JOSE DO PIAUÍ - PI.', 'PRESTAÇÃO DE SERVIÇOS DE TROCA DE PISO DA SECRETARIA MUNICIPAL DE SAÚDE (SEMUSA) DE DEMERV AL LOBÃO - PIAUÍ.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cljjo3loSgx5",
        "outputId": "b3353cb3-636f-4f5b-e8ae-38b424904cb7"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "sw = stopwords.words('portuguese')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nriFr2jNpZNE",
        "outputId": "23d02719-b521-499d-b388-5124ed25d253"
      },
      "source": [
        "#Importar nltk\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOWwc02EMe5t"
      },
      "source": [
        "#Tokenizar dados e remover stopwords\n",
        "\n",
        "new_sw = (['atender', 'novo', 'contrato', 'saúde', 'coronavírus', 'combate', 'secretaria', 'objeto',\n",
        "           'covid', 'pública', '2020', 'municipal', 'pi','necessidades', '00', '19',\n",
        "           'pandemia', 'ações', 'coronavirus', 'município', 'municipio', 'termo', 'emergencial', 'piauí',\n",
        "           'emergência', 'nº', 'interesse', 'presente', 'decorrente', 'fornecimento', 'valor', 'conforme',\n",
        "           'aditivo', 'caráter', 'destinados', 'medidas', 'atendimento', 'prevenção', 'necessidade',\n",
        "           'excepcional', 'lei', 'pacientes', 'enfrentamento'])\n",
        "sw.extend(new_sw)\n",
        "#sw.append(['secretaria', 'municipal', 'pi', 'saúde', 'covid']) \n",
        "#'prestação', 'contratação', 'aquisição', 'serviços' \n",
        "\n",
        "def tokenizar(corpus):\n",
        "  tokens = []\n",
        "  for objeto in corpus:\n",
        "      tokens_objeto = nltk.word_tokenize(objeto, language='portuguese')\n",
        "      tokens_objeto = [token.lower() for token in tokens_objeto]\n",
        "      for token in tokens_objeto:\n",
        "          tokens.append(token)\n",
        "  tokens_menos_sw = []\n",
        "  for t in tokens:\n",
        "    if t not in sw and t.isalpha() :\n",
        "      tokens_menos_sw.append(t)\n",
        "  return tokens_menos_sw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8p6DwF6d3JUR",
        "outputId": "94b0b349-9746-480c-8ac0-679355db3f00"
      },
      "source": [
        "dados_obj = tokenizar(corpus)\n",
        "dados_obj_simpl = tokenizar(corpus_obj_simp)\n",
        "\n",
        "dados_totais = dados_obj + dados_obj_simpl\n",
        "\n",
        "print(len(dados_obj))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "77595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCD15m0NL9LL",
        "outputId": "b5bb41e1-8300-4d02-a120-776436f0c540"
      },
      "source": [
        "#Binário\n",
        "import random\n",
        "\n",
        "#Pegando as palavras mais frequentes\n",
        "allWords = nltk.FreqDist(w.lower() for w in dados_obj)\n",
        "\n",
        "\n",
        "#key_words = list(allWords)[0:20]\n",
        "key_words = ('teste', 'testes', 'rápidos', 'igg', 'igm', 'alimentação', 'reforma', 'medicamentos', \n",
        "             'especializada', 'locação', 'serviço', 'serviços', 'cargo', 'epi', 'epis', 'epi\\'s', \n",
        "             'máscara' 'máscaras', 'materiais', 'empresa', 'luvas', 'leitos', 'limpeza', 'higiene', \n",
        "             'hospitalar', 'enfermeiro', 'médico', 'álcool', 'gel', 'insumos', 'veículos', 'equipamentos', \n",
        "             'física', 'pessoa', 'temporário', 'função')\n",
        "\n",
        "def corpus_features(doc):\n",
        "  i = 1\n",
        "# doc_words = [w for w in dados_obj]\n",
        "  doc_words = key_words\n",
        "  feats = {}\n",
        "  #if i == 1:\n",
        "    #print(doc_words)\n",
        "    #i = i + 1\n",
        "  for word in doc_words:\n",
        "    if (word in doc):\n",
        "      feats[f'countains({word})'] = True\n",
        "#      feats[f'countains({word})'] = (word in doc)\n",
        "  return feats\n",
        "\n",
        "labeled_names = []\n",
        "for i in range(len(corpus)):\n",
        "#    print(i, corpus[i], corpus_obj_simp[i])\n",
        "    labeled_names.append([corpus[i], corpus_obj_simp[i]])  \n",
        "\n",
        "#labeled_names = ([(name, 'obj') for name in dados_obj[0:5000]] + [(name, 'obj_simpl') for name in dados_obj_simpl[0:5000]])\n",
        "print(labeled_names[-5:])\n",
        "random.shuffle(labeled_names)\n",
        "\n",
        "featuresets = [(corpus_features(n), cat) for (n, cat) in labeled_names]\n",
        "train_set, test_set = featuresets[0:5400], featuresets[5400:]\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "print(\"Taxa de acurácia da classificação: \",nltk.classify.accuracy(classifier, test_set))\n",
        "print(classifier.show_most_informative_features())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Contratação de Empresa para fornecimentos de Teste Rápido para detecção especifica de IGG e IGM do COVID-19, para o Município de Dom Inocêncio - PI.', 'Aquisição de testes para diagnóstico de COVID-19'], ['Prestação de contrataçao de profissional para dar suporte técnico ao desenvolvimento do trabalho de: Acompanhamento a Lei Emergenclal Cultural Aldir Biano, sancionada através do Decreto Federal no 14.017/2020; Alimentação dos sistemas e plataformas necessárias; acompanhamento e julgamento dos editais, todo suporte técnico ao Setor de Cultura junto a Secretaria Municipal de Cultura, Esporte e Lazer e a Prefeitura Municipal de São Jose do Piaul - PI.', 'Contratação de prestador de serviço - pessoa física'], ['AQUISIÇÃO DE PULVERIZADOR 20 LT FIBRA BICO PLASTICO PARA SECRETARIA MUNICIPAL DE EDUCAÇÃO E ESCOLAS DA REDE ESTADUAL E MUNICIPAL DO MUNICÍPIO DE SÃO JOSÉ DO PIAUÍ· PI.', 'Aquisição de equipamentos diversos'], ['AQUISIÇÃO DE ÁGUA SANITÁRIA I LT, DETERGENTE 500 ML E PAPEL TOALHA PARA SECRETARIA MUNICIPAL DE EDUCAÇÃO E SEUS RESPECTNOS ÓRGÃOS DO MUNICtPIO DE SÃO JOSE DO PIAUÍ - PI.', 'Aquisição de material de limpeza'], ['PRESTAÇÃO DE SERVIÇOS DE TROCA DE PISO DA SECRETARIA MUNICIPAL DE SAÚDE (SEMUSA) DE DEMERV AL LOBÃO - PIAUÍ.', 'Construção, reforma e/ou ampliação de unidade hospitalar']]\n",
            "Taxa de acurácia da classificação:  0.5025316455696203\n",
            "Most Informative Features\n",
            "       countains(testes) = True           Aquisi : Contra =    381.6 : 1.0\n",
            "      countains(locação) = True           Locaçã : Contra =    341.2 : 1.0\n",
            "        countains(teste) = True           Aquisi : Contra =    277.7 : 1.0\n",
            "    countains(materiais) = True           Aquisi : Contra =    182.1 : 1.0\n",
            "          countains(gel) = True           Aquisi : Contra =    170.9 : 1.0\n",
            "      countains(empresa) = True           Aquisi : Contra =    164.3 : 1.0\n",
            " countains(equipamentos) = True           Aquisi : Contra =    163.6 : 1.0\n",
            "       countains(física) = True           Locaçã : Aquisi =    139.4 : 1.0\n",
            "       countains(leitos) = True           Locaçã : Aquisi =    101.3 : 1.0\n",
            "      countains(rápidos) = True           Aquisi : Aquisi =     97.5 : 1.0\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pn_5o_Y3L81r",
        "outputId": "3bb5b8ef-9602-4c64-a60b-79a97ac81bd8"
      },
      "source": [
        "#Formato TF\n",
        "\n",
        "\n",
        "# Cria o objeto vetorizador\n",
        "vectorizer = CountVectorizer(analyzer='word', binary=False, decode_error='strict', encoding='utf-8', input='content',\n",
        "                lowercase=True, max_df=1.0, max_features=20, min_df=1,\n",
        "                ngram_range=(1, 1), preprocessor=None, stop_words=sw,\n",
        "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
        "                tokenizer=None, vocabulary=None)\n",
        "\n",
        "\n",
        "# Aplica o vetorizador nos dados\n",
        "vectorizer.fit(corpus)\n",
        "\n",
        "\n",
        "# Saída:\n",
        "\n",
        "print(\"Temos um dicionário com o termo e suas respectivas frequências, ou seja quantas vezes aquele termo apareceu na base de dados:\\n\",vectorizer.vocabulary_)\n",
        "\n",
        "# Aplicando o transformador\n",
        "matrix = vectorizer.transform(corpus)\n",
        "\n",
        "# Visualizando a forma da matriz gerada\n",
        "print(\"\\nSHAPE:\",matrix.shape)\n",
        "\n",
        "# Imprimindo a matriz binária\n",
        "print(\"\\n\",matrix.toarray())\n",
        "\n",
        "#esse vetor tem muitos zeros, por essa razão eles são chamados de vetores esparsos."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temos um dicionário com o termo e suas respectivas frequências, ou seja quantas vezes aquele termo apareceu na base de dados:\n",
            " {'empresa': 1, 'prestação': 13, 'serviço': 17, 'aquisição': 0, 'equipamentos': 3, 'médico': 12, 'material': 10, 'hospitalar': 7, 'hospital': 6, 'especializada': 4, 'materiais': 9, 'serviços': 18, 'profissionais': 14, 'medicamentos': 11, 'público': 16, 'proteção': 15, 'individual': 8, 'horas': 5, 'enfermagem': 2, 'social': 19}\n",
            "\n",
            "SHAPE: (6190, 20)\n",
            "\n",
            " [[0 1 0 ... 1 0 0]\n",
            " [1 1 0 ... 0 0 0]\n",
            " [1 1 0 ... 0 0 0]\n",
            " ...\n",
            " [1 0 0 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ub90D4OpMWTz",
        "outputId": "70664c32-1f50-4b3d-fbc0-22706b7c7af1"
      },
      "source": [
        "#Formato Tf-idf\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "# Instanciar o objeto TFIDF \n",
        "vectorizer = TfidfVectorizer(analyzer='word', binary=False, decode_error='strict', encoding='utf-8',\n",
        "                input='content', lowercase=True, max_df=1.0, max_features=20,\n",
        "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
        "                smooth_idf=True, stop_words=sw, strip_accents=None,\n",
        "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
        "                tokenizer=None, use_idf=True, vocabulary=None)\n",
        "\n",
        "\n",
        "# Constroi o vetorizador nos dados\n",
        "vectorizer.fit(corpus)\n",
        "\n",
        "# saída:\n",
        "\n",
        "# Imprime o vocabulario\n",
        "print(vectorizer.vocabulary_)\n",
        "\n",
        "print(\"\\n\",\"Essas são as pontuações que foram dadas para cada um dos termos da base de dados. As palavras mais relevantes tem uma pontuação maior:\\n\",vectorizer.idf_)\n",
        "\n",
        "# Aplicando aos dados\n",
        "matrix = vectorizer.transform(corpus)\n",
        "\n",
        "# Imprime a forma da matrix\n",
        "print(\"\\n\",\"SHAPE:\",matrix.shape)\n",
        "print(\"\\n\",matrix.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'empresa': 1, 'prestação': 13, 'serviço': 17, 'aquisição': 0, 'equipamentos': 3, 'médico': 12, 'material': 10, 'hospitalar': 7, 'hospital': 6, 'especializada': 4, 'materiais': 9, 'serviços': 18, 'profissionais': 14, 'medicamentos': 11, 'público': 16, 'proteção': 15, 'individual': 8, 'horas': 5, 'enfermagem': 2, 'social': 19}\n",
            "\n",
            " Essas são as pontuações que foram dadas para cada um dos termos da base de dados. As palavras mais relevantes tem uma pontuação maior:\n",
            " [2.20845167 2.86287749 3.97195013 3.78543129 3.96253091 4.21339901\n",
            " 3.51624381 4.05752864 4.18177582 3.91374074 3.57387292 3.82821857\n",
            " 4.21339901 2.5919849  3.76470516 3.64180703 4.19351764 3.68584659\n",
            " 2.26806275 4.03711976]\n",
            "\n",
            " SHAPE: (6190, 20)\n",
            "\n",
            " [[0.         0.53626686 0.         ... 0.69042331 0.         0.        ]\n",
            " [0.32864902 0.42603689 0.         ... 0.         0.         0.        ]\n",
            " [0.32864902 0.42603689 0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [1.         0.         0.         ... 0.         0.         0.        ]\n",
            " [1.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.6585171  0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZxY2UOXgo1f"
      },
      "source": [
        "### Conclusão\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqIiAf4V-ho6"
      },
      "source": [
        "Foi criado o corpus a partir da célula \"Objeto\" da planilha dos diários oficiais referenciada no início do trabalho. Desse corpus, retiramos as stopwords da lígua porteguesa. Retiramos também, as palavras que entendíamos não ter relevância na classificação dos objetos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqAQEg0PBM8W"
      },
      "source": [
        "Na representação de features binárias, escolhemos um conjunto de palavras, que consideramos serem relevantes na classificação dos objetos para as classes de \"objetos simplificados\", e criamos uma feature se havia ou não a palavra no documento. Nessa representação, criamos um modelo experimental utilizando o método Navie Bayes como classificador. Separamos a base treinamento e base de teste e colocamos nosso modelo para executar algumas vezes. Na última versão, a acurácia variava de 0,45 a 0,5025, sendo que nas versões iniciais desse modelo a acurácia estava por volta de 0,35. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZn1QRUgHzCi"
      },
      "source": [
        "Por fim, implementamos as representações TF e TF-IDF. Inicicialmente, nos dois casos, configuramos o número de features muito grande (500), o que fazia com o que nossa matriz só desse para visualizar 0s. Com a diminuição desse parâmetro para 20, conseguimos visualizar o funcionamento do método, identificando as palavras que continham nos documentos, tanto na frequência das palavras no TF, quanto no cálculo do TF-IDF."
      ]
    }
  ]
}